# -*- coding: utf-8 -*-
"""Ch14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q9qN9XUC5G8wrTB6Hp0BkyMm38mul5sf
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import scipy.io
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

!ls /content/drive/MyDrive/data/

!wget -qq https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/wiki_crop.tar
!tar -xf wiki_crop.tar

metadata = scipy.io.loadmat("wiki_crop/wiki.mat")
df = pd.DataFrame(columns=["dob", "photo_taken", "full_path", "name", "face_score",  "second_face_score"])
df["dob"] = metadata["wiki"][0, 0]["dob"][0]
df["photo_taken"] = metadata["wiki"][0, 0]["photo_taken"][0]
df["full_path"] = ["wiki_crop/"+i[0] for i in metadata["wiki"][0, 0]["full_path"][0]]
df["name"] = metadata["wiki"][0, 0]["name"][0]
df["face_score"] = metadata["wiki"][0, 0]["face_score"][0]
df["second_face_score"] = metadata["wiki"][0, 0]["second_face_score"][0]

df = df[df["second_face_score"].isna()]
df.shape

df = df[df["face_score"] > min(df["face_score"])]
#df = df.drop(["face_score","second_face_score"], axis=1)

def get_age(taken, dob):
    birth = datetime.fromordinal(int(dob)) + timedelta(days=dob%1) - timedelta(days = 366)
    if birth.month < 7:
        return taken - birth.year
    else:
        return taken - birth.year - 1
df["age"] = df.apply(lambda x: get_age(x["photo_taken"], x["dob"]), axis=1)

df=df[(df.age<100) & (df.age>0)]

def get_age_group(age):
    if 0 < age <= 18:
      return 0
    elif 19 < age <= 29:
      return 1
    elif 30 < age <= 39:
      return 2
    elif 40 < age <= 49:
      return 3
    elif 50 < age <= 59:
      return 4
    else:
      return 5
df["age_group"] = df["age"].map(get_age_group)
df = df[df.face_score > 3.23]
df.shape

df.groupby("age_group").count()["age"].plot(kind="bar")

BATCH_SIZE = 128
IMAGE_SHAPE = (64,64,3)
NUM_OF_CLASSES = 6
LATENT_DIM = 100
EPOCHS = 50

def create_discriminator():
    inputs_label = tf.keras.Input(shape=(1,))
    x = tf.keras.layers.CategoryEncoding(num_tokens=NUM_OF_CLASSES, output_mode="one_hot")(inputs_label)
    units = IMAGE_SHAPE[0] * IMAGE_SHAPE[1]
    x = tf.keras.layers.Dense(units)(x)
    labels = tf.keras.layers.Reshape((IMAGE_SHAPE[0], IMAGE_SHAPE[1], 1))(x)

    inputs_image = tf.keras.layers.Input(shape=IMAGE_SHAPE)
    x = tf.keras.layers.Concatenate()([inputs_image, labels])
    x = tf.keras.layers.Conv2D(128, (3,3), strides=(2,2), padding='same')(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    x = tf.keras.layers.Conv2D(128, (3,3), strides=(2,2), padding='same')(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)
    x = tf.keras.layers.Dropout(0.4)(x)
    x = tf.keras.layers.Flatten()(x)
    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)

    model = tf.keras.Model([inputs_image, inputs_label], outputs)
    opt = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

def create_generator():
    inputs_label = tf.keras.Input(shape=(1,))
    x = tf.keras.layers.CategoryEncoding(num_tokens=NUM_OF_CLASSES, output_mode="one_hot")(inputs_label)
    units = 16 * 16
    x = tf.keras.layers.Dense(units)(x)
    labels = tf.keras.layers.Reshape((16, 16, 1))(x)

    inputs_lat = tf.keras.Input(shape=(LATENT_DIM,))
    units = 128 * 16 * 16
    x = tf.keras.layers.Dense(units)(inputs_lat)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)
    latent = tf.keras.layers.Reshape((16, 16, 128))(x)

    x = tf.keras.layers.Concatenate()([latent, labels])
    x = tf.keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)
    x = tf.keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(x)
    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)
    outputs = tf.keras.layers.Conv2D(3, (7,7), activation='tanh', padding='same')(x)
    model = tf.keras.Model([inputs_lat, inputs_label], outputs)
    return model

def create_cgan(g_model, d_model):
    d_model.trainable = False
    gen_noise, gen_label = g_model.input
    gen_output = g_model.output
    gan_output = d_model([gen_output, gen_label])
    model = tf.keras.Model([gen_noise, gen_label], gan_output)
    opt = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)
    model.compile(loss='binary_crossentropy', optimizer=opt)
    return model

#tf.keras.utils.plot_model(gan_model, show_shapes=True)
def preprocess_data(img_path, trainy):
    img = tf.io.read_file(img_path)
    img = tf.io.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (IMAGE_SHAPE[0], IMAGE_SHAPE[1]))
    img = (img - 127.5) / 127.5
    return [img, trainy]

def create_dataset(images, trainy):
    dataset = tf.data.Dataset.from_tensor_slices((images, trainy))
    dataset = dataset.map(preprocess_data, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.shuffle(BATCH_SIZE*8).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    return dataset

def create_latent_points(n_samples):
	x_input = np.random.randn(LATENT_DIM * n_samples)
	z_input = x_input.reshape(n_samples, LATENT_DIM)
	labels = np.random.randint(0, NUM_OF_CLASSES, n_samples)
	return [z_input, labels]

def create_fake_data(generator):
	z_input, labels_input = create_latent_points(BATCH_SIZE)
	images = generator.predict([z_input, labels_input])
	y = np.zeros((BATCH_SIZE, 1))
	return [images, labels_input], y

def train(g_model, d_model, gan_model, dataset):
    for i in range(EPOCHS):
        b=0
        for batch in dataset:
            if batch[0].shape[0] < BATCH_SIZE:
                continue
            print("Epoch: {0}, Batch: {1}".format(i, b))
            b+=1
            X_real = batch[0]
            labels_real = batch[1]
            y_real = np.ones((BATCH_SIZE, 1))

            real_data_loss, _ = d_model.train_on_batch([X_real, labels_real], y_real)

            [X_fake, labels], y_fake = create_fake_data(g_model)
            fake_data_loss, _ = d_model.train_on_batch([X_fake, labels], y_fake)

            [z_input, labels_input] = create_latent_points(BATCH_SIZE)
            y_gan = np.ones((BATCH_SIZE, 1))

            gan_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)
            print("d1={0:.3f}, d2={1:.3f} g={2:.3f}".format(real_data_loss, fake_data_loss, gan_loss))
    g_model.save('cgan_generator.h5')

d_model = create_discriminator()
g_model = create_generator()
gan_model = create_cgan(g_model, d_model)
dataset = create_dataset(np.array(df["full_path"]), np.array(df["age_group"]))
train(g_model, d_model, gan_model, dataset)

!cp cgan_generator.h5 /content/drive/MyDrive/data/

def create_plot(examples, n):
    for i in range(n * n):
        plt.subplot(n, n, 1 + i)
        plt.axis('off')
        plt.imshow(examples[i, :, :])
    plt.show()

model = tf.keras.saving.load_model('cgan_generator.h5')
latent_points, labels = create_latent_points(36)
labels = np.asarray([x for _ in range(6) for x in range(6)])
X  = model.predict([latent_points, labels])
X = (X + 1) / 2.0
create_plot(X, 6)

