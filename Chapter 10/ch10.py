# -*- coding: utf-8 -*-
"""Ch10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nINr-Ea-36yxgpnw-pYcMQepcWkGapEr

# CFPB
"""

#!wget https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/?date_received_max=2023-05-13&date_received_min=2022-11-13&field=all&format=csv&no_aggs=true&product=Debt%20collection&product=Credit%20card%20or%20prepaid%20card&product=Checking%20or%20savings%20account&size=76696&sort=created_date_desc

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/MyDrive/data/

import pandas as pd

df = pd.read_csv("complaints.csv")

df.head(2)

df.shape

df = df[['Product', 'Issue', 'Sub-issue', 'Consumer complaint narrative']]

df.isna().sum()

df.shape

df = df.dropna()
df.shape

df.columns

df["Consumer complaint narrative"][0]

df["Complaints"] = df["Issue"].map(str) + ". " + df["Sub-issue"].map(str) + ". " + df["Consumer complaint narrative"].map(str)

df  = df.drop(["Issue", "Sub-issue", "Consumer complaint narrative"], axis=1)

df['Complaints'] = df['Complaints'].str.lower()
df.head()

import nltk 
nltk.download('stopwords')
nltk.download('punkt')

from nltk.stem import PorterStemmer
ps = PorterStemmer()

df.Complaints[1]

df["Complaints"] = df["Complaints"].str.replace("[^a-z ]", " ", regex=True)
df["Complaints"] = df["Complaints"].str.replace("[x+]", " ", regex=True)

df.Complaints[1]

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

all_words = []

def get_clean_text(text):
    words = text.split()
    if len(words)>10 and len(words)<=1000:
        #clean_word_list = [ps.stem(word.strip()) for word in words]
        clean_word_list = [lemmatizer.lemmatize(word.strip()) for word in words]
        all_words.extend(clean_word_list)
        return " ".join(clean_word_list)
    else:
        return None

df['clean_text'] = df['Complaints'].apply(get_clean_text)

df['clean_text'][1]

print(df.shape)
df = df.dropna()
print(df.shape)

df["Product"].unique()

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

le = LabelEncoder()
df["target"] = le.fit_transform(df["Product"])

X_train, X_test, y_train, y_test = train_test_split(df["clean_text"], df["target"], test_size=0.3)

df[["Product", "target"]].head()

all_words = list(set(all_words))
len(all_words)

import tensorflow as tf

X_train = tf.convert_to_tensor(X_train)
X_test = tf.convert_to_tensor(X_test)
BATCH_SIZE = 64

train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

VOCAB_SIZE = len(all_words)

encoder = tf.keras.layers.TextVectorization(
    max_tokens=VOCAB_SIZE)

encoder.adapt(train_ds.map(lambda text, label: text))

print(encoder.get_vocabulary())

"""# LSTM"""

model = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary()),
        output_dim=64,
        mask_zero=True),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(3)
])

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['sparse_categorical_accuracy'])

history = model.fit(train_ds, epochs=50,
                    validation_data=test_ds,
                    validation_steps=30, verbose=2)

test_loss, test_acc = model.evaluate(test_ds)

print('Test Loss:', test_loss)
print('Test Accuracy:', test_acc)

comment = tf.constant(["i did not activate my credit card but there was a transaction of amount"])
comment_dataset = tf.data.Dataset.from_tensor_slices((comment)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

model.predict(comment_dataset)

"""# GRU"""

model = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary()),
        output_dim=64,
        mask_zero=True),
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(3)
])

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['sparse_categorical_accuracy'])

history = model.fit(train_ds, epochs=50,
                    validation_data=test_ds,
                    validation_steps=30, verbose=2)

test_loss, test_acc = model.evaluate(test_ds)

print('Test Loss:', test_loss)
print('Test Accuracy:', test_acc)

"""# Conv 1D"""

model = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary()),
        output_dim=64,
        mask_zero=True),
    tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(3)
])

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['sparse_categorical_accuracy'])

model.summary()

history = model.fit(train_ds, epochs=10,
                    validation_data=test_ds,
                    validation_steps=30, verbose=2)

test_loss, test_acc = model.evaluate(test_ds)

print('Test Loss:', test_loss)
print('Test Accuracy:', test_acc)



from gensim.models.doc2vec import Doc2Vec, TaggedDocument
train_tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(X_train.str.split())]
test_tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(X_test.str.split())]

d2v_model = Doc2Vec(train_tagged_data, vector_size = 50, window = 2, min_count = 1, workers = 2)

X_train_vector = d2v_model.dv.vectors
X_test_vector = np.array([d2v_model.infer_vector(doc.words) for doc in test_tagged_data])

X_train_tensor = tf.convert_to_tensor(X_train_vector.reshape((X_train_vector.shape[0], 1, X_train_vector.shape[1])))
X_test_tensor = tf.convert_to_tensor(X_test_vector.reshape((X_test_vector.shape[0], 1, X_test_vector.shape[1])))
BATCH_SIZE = 64

train_ds = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
test_ds = tf.data.Dataset.from_tensor_slices((X_test_tensor, y_test)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

model = tf.keras.Sequential([
    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(50)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(3)
])

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['sparse_categorical_accuracy'])

history = model.fit(train_ds, epochs=10,
                    validation_data=test_ds,
                    validation_steps=30, verbose=2)

"""# GLoVe"""

!wget "http://nlp.stanford.edu/data/glove.6B.zip"
!unzip "glove.6B.zip"

from gensim.models import KeyedVectors

glove_input_file = "glove.6B.50d.txt"
embedding_model = KeyedVectors.load_word2vec_format(glove_input_file, no_header=True)

word_to_index = dict()
vocab = encoder.get_vocabulary()

for i in range(len(vocab)):
    word_to_index[vocab[i]] = i

word_to_index

import numpy as np

word_vector_dim = 50
embedding_matrix = np.zeros((len(vocab), word_vector_dim))
miss = 0

for key in word_to_index.keys():
    if key in embedding_model:
        word_emb_vector = embedding_model[key]
        embedding_matrix[i] = word_emb_vector
    else:
        print(key)
        miss+=1

# Words not found in embedding index will be all-zeros.

miss

embedding_matrix.shape

len(vocab)

model = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary()),
        output_dim=50,
        #embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),
        weights=[embedding_matrix],
        trainable=False),
    tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(3)
])

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['sparse_categorical_accuracy'])

history = model.fit(train_ds, epochs=50,
                    validation_data=test_ds,
                    validation_steps=30, verbose=2)

"""# NNLM"""

import tensorflow_hub as hub
embedding_url = "https://tfhub.dev/google/nnlm-en-dim50/2"
embedding_layer = hub.KerasLayer(embedding_url, input_shape=[], dtype=tf.string, trainable=True)

model = tf.keras.Sequential([
    embedding_layer
])
res = model.predict(["i visited that place once"])
res

res.shape



model = tf.keras.Sequential([
    embedding_layer,
    tf.keras.layers.Reshape((1, 50)),
    tf.keras.layers.GRU(64),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(3)
])

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['sparse_categorical_accuracy'])

history = model.fit(train_ds, epochs=50,
                    validation_data=test_ds,
                    validation_steps=30, verbose=2)

test_loss, test_acc = model.evaluate(test_ds)

print('Test Loss:', test_loss)
print('Test Accuracy:', test_acc)

model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=['sparse_categorical_accuracy'])

history = model.fit(train_ds, epochs=50,
                    validation_data=test_ds,
                    validation_steps=30, verbose=2)

test_loss, test_acc = model.evaluate(test_ds)

print('Test Loss:', test_loss)
print('Test Accuracy:', test_acc)

