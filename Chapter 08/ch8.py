# -*- coding: utf-8 -*-
"""Ch8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16kn-74YxmKycuuxO82GeeQtQ11iPTdle
"""

import nltk

nltk.download('punkt')

from nltk.tokenize import word_tokenize
text = "Good afternoon Mr. Ray. May I bring something for you?"
tokens = word_tokenize(text) 
tokens

from nltk.tokenize import sent_tokenize

text = """The city yesterday witnessed the hottest day of the century. The wait for rain may end today. 
        Though most of the day would be warm and sunny, a drizzle is expected in the evening."""

tokens = sent_tokenize(text) 
tokens

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords

print(stopwords.words("english"))

print(len(stopwords.words("english")))

text = """The city yesterday witnessed the hottest day of the century. The wait for rain may end today. 
        Though most of the day would be warm and sunny, a drizzle is expected in the evening."""

words = list()

for word in word_tokenize(text.lower()):
    if word not in stopwords.words("english"):
        words.append(word)

words

words = [word for word in word_tokenize(text.lower()) if word not in stopwords.words("english")]
words

text = """The city yesterday witnessed the hottest day of the century. The wait for rain may end today. 
        Though most of the day would be warm and sunny, a drizzle is expected in the evening."""

clean_sentences = list()

for sentence in sent_tokenize(text.lower()):
    words = [word for word in word_tokenize(sentence) if word not in stopwords.words("english")]
    clean_sent = " ".join(words)
    clean_sentences.append(clean_sent)

clean_sentences

from nltk.stem.porter import PorterStemmer

text = """Yesterday we played Cricket at the school. Today we are playing Football. 
        Tomorrow we will play Tennis."""

stemmer = PorterStemmer()

stemmed = [stemmer.stem(word) for word in word_tokenize(text.lower())]
stemmed

word = "went"
stemmed_word = stemmer.stem(word)
print(stemmed_word)

nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

print("The lemma of the word 'trees' is:", lemmatizer.lemmatize("trees"))
print("The lemma of the word 'matrices' is:", lemmatizer.lemmatize("matrices"))

print("The lemma of the word 'walking':", lemmatizer.lemmatize("walking"))
print("The lemma of the word 'walking' as verb:", lemmatizer.lemmatize("walking", pos ="v"))

nltk.download('tagsets')
nltk.download('averaged_perceptron_tagger')

sent = "a quick brown fox jumps over the lazy dog"
words = word_tokenize(sent)
nltk.tag.pos_tag(words)

nltk.help.upenn_tagset()

"""# spaCy"""

!python -m spacy download en_core_web_sm

import spacy

nlp = spacy.load('en_core_web_sm')
doc = nlp("It was the best of times. It was the worst of times.")

print("Word tokenizer.")
print("*"*30)
for token in doc:
    print(token.text)

print("Sentence tokenizer.")
print("*"*30)
for sent in doc.sents:
    print(sent.text)

print(type(doc))
print(type(token))

import pathlib
input_file = "file.txt"
doc = nlp(pathlib.Path(input_file).read_text(encoding="utf-8"))

doc = nlp("I know, you take this medicine 3 times in a day.")

for token in doc:
    print("The token: \"{0}\", is digit: {1}, is alpha: {2}, is punctuation: {3}"
          .format(token.text, token.is_digit, token.is_alpha, token.is_punct))

for token in doc:
    if not token.is_stop:
        print(token.text)

sent = "a quick brown fox jumps over the lazy dog"
doc = nlp(sent)

for token in doc:
    print(token.text, token.pos_)

doc = nlp("""Elon Musk is the CEO of Tesla. 
             It is an American multinational automotive company headquartered in Austin, Texas.
             It generated a revenue of US$24.3 billion in Q4 2022.""")

for ent in doc.ents:
    print("{0} --> {1}".format(ent.text, ent.label_))

nlp.pipe_labels['ner']

sent = "a quick brown fox jumps over the lazy dog"
doc = nlp(sent)

for token in doc:
    print("Token: \"{0}\", Syntactic dependency label: {1}, \
    Predicted head: {2}, POS tag: {3}, Syntactic children: {4}".format(token.text, token.dep_, 
                                                   token.head.text, token.head.pos_, 
                                                   [child for child in token.children]))

from spacy import displacy

sent = "a quick brown fox jumps over the lazy dog"
doc = nlp(sent)
displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})

doc = nlp("""Elon Musk is the CEO of Tesla. 
             It is an American multinational automotive company headquartered in Austin, Texas.
             It generated a revenue of US$24.3 billion in Q4 2022.""")
displacy.render(doc, style='ent', jupyter=True, options={'distance': 90})

text = ("the tourists went to the forest yesterday. today they are going to the temple.")
doc = nlp(text)
for token in doc:
    if token.text != token.lemma_:
        print("Token: \"{0}\" --> Lemma: \"{1}\"".format(token.text, token.lemma_))

from collections import Counter
long_text = (
    """A computer is a machine that can be programmed to carry out 
    sequences of arithmetic or logical operations (computation) automatically. 
    Modern digital electronic computers can perform generic sets of operations known as programs. 
    These programs enable computers to perform a wide range of tasks. 
    A computer system is a nominally complete computer that includes the hardware, 
    operating system (main software), and peripheral equipment needed and used for full operation. 
    This term may also refer to a group of computers that are linked and function together, 
    such as a computer network or computer cluster.

    A broad range of industrial and consumer products use computers as control systems. 
    Simple special-purpose devices like microwave ovens and remote controls are included, 
    as are factory devices like industrial robots and computer-aided design, as well as 
    general-purpose devices like personal computers and mobile devices like smartphones. 
    Computers power the Internet, which links billions of other computers and users.""")

doc = nlp(long_text)

words = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop and not token.is_punct]
print(Counter(words).most_common(5))

!python -m spacy download en_core_web_md

nlp = spacy.load("en_core_web_sm") 

doc1 = nlp("Hello, how are you today?")
doc2 = nlp("Hi, how is your day going?")

similarity = doc1.similarity(doc2)
print(similarity)

nlp = spacy.load("en_core_web_md") 

doc1 = nlp("Hello, how are you today?")
doc2 = nlp("Hi, how is your day going?")

similarity = doc1.similarity(doc2)
print(similarity)

"""# Word Embedding"""

from sklearn.preprocessing import OneHotEncoder
import numpy as np

text = "the sun may rise in the east, at least its settled in the final location"
text = np.array(text.split()).reshape((-1, 1))

ohe = OneHotEncoder()
ohe.fit_transform(text).toarray()

print(ohe.categories_[0])

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()

sample_text = ["""The Moon is Earth's only natural satellite. It is the fifth largest 
  satellite in the Solar System and the largest and most massive relative to its parent planet, 
  with a diameter about one-quarter that of Earth (comparable to the width of Australia).
  The Moon is a planetary-mass object with a differentiated rocky body, making it a satellite planet 
  under the geophysical definitions of the term and larger than all known dwarf planets of the Solar System. 
  It lacks a significant atmosphere, hydrosphere, or magnetic field. The Moon orbits Earth at an average 
  distance of 384,400 km (238,900 mi), or about 30 times Earth's diameter."""]

vectorizer.fit(sample_text)
print('Vocabulary: ')
print(vectorizer.vocabulary_)

vector = vectorizer.transform(sample_text)

print(vectorizer.transform(["The Earth and the Moon form a satellite system"]).toarray())

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
sample_text = ["""The Moon is Earth's only natural satellite. It is the fifth largest 
  satellite in the Solar System and the largest and most massive relative to its parent planet, 
  with a diameter about one-quarter that of Earth.The Moon is a planetary-mass object with a 
  differentiated rocky body, making it a satellite planet under the geophysical definitions 
  of the term and larger than all known dwarf planets of the Solar System. 
  It lacks a significant atmosphere, hydrosphere, or magnetic field."""]

vectorizer.fit(sample_text)
print('Vocabulary: ')
print(vectorizer.vocabulary_)

vector = vectorizer.transform(sample_text)
print(vectorizer.transform(["The Earth is a planet."]).toarray())
print(vectorizer.transform(["The Earth and the Moon form a satellite system"]).toarray())

"""# Gensim"""

import pathlib
import spacy

nlp = spacy.load('en_core_web_sm')
input_file = "book.txt"
doc = nlp(pathlib.Path(input_file).read_text(encoding="utf-8"))

data = []
for sent in doc.sents:
    word_tokens = []
    for token in sent:
        if token.is_alpha:
            word_tokens.append(token.text.lower())
    data.append(word_tokens)

import gensim
from gensim.models import Word2Vec

cbow_model = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, 
                                    window = 5, sg = 0)
 
cbow_model.wv.get_vector("king")

cbow_model.wv.most_similar('king', topn=10)

print(cbow_model.wv.similarity("king", "emperor"))

import numpy as np
from numpy.linalg import norm

X = cbow_model.wv.get_vector("king")
Y = cbow_model.wv.get_vector("emperor")
cosine = np.dot(X,Y)/(norm(X)*norm(Y))
print("Cosine Similarity:", cosine)

skipgram_model = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100,
                                             window = 5, sg = 1)
print(skipgram_model.wv.similarity('king', 'emperor'))
skipgram_model.wv.most_similar("king")

skipgram_model.save("skipgram_model")

model = Word2Vec.load("skipgram_model")
model.wv.most_similar("queen")

from gensim.models import KeyedVectors

skipgram_model.wv.save("emb_vectors")

word_vectors = KeyedVectors.load("emb_vectors", mmap='r')
word_vectors["island"]

import gensim

def tagged_document(data):
   for i, doc in enumerate(data):
      yield gensim.models.doc2vec.TaggedDocument(doc, [i])

training_data = list(tagged_document(data))

model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=1, epochs=30, window=5)
model.build_vocab(training_data)
model.train(training_data, total_examples=model.corpus_count, epochs=model.epochs)

model.infer_vector(['the', 'island', 'is', 'beautiful'])

"""# TensorFlow"""

import pathlib
import spacy

nlp = spacy.load('en_core_web_sm')
input_file = "book.txt"
doc = nlp(pathlib.Path(input_file).read_text(encoding="utf-8"))

vocab = []
for token in doc:
    if token.is_alpha:
        vocab.append(token.text.lower())
vocab = list(set(vocab))

import tensorflow as tf
dataset = tf.data.Dataset.from_tensor_slices(vocab)

vectorizer = tf.keras.layers.TextVectorization(
    max_tokens=len(vocab),
    output_mode='int',
    output_sequence_length=10)


vectorizer.adapt(dataset.batch(64))

model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
model.add(vectorizer)

input_data = [["i do not struggle for triumph"]]
model.predict(input_data)

for ind in model.predict(input_data)[0]:
    print(vectorizer.get_vocabulary()[ind])

from tensorflow.keras.layers import Embedding
embedding_layer = Embedding(input_dim=len(vocab), output_dim=16, input_length=10)
embedding_layer(model.predict(input_data))

embedding_layer = Embedding(input_dim=len(vocab), output_dim=16, input_length=10)
A = embedding_layer(model.predict([["boy"]]))
B = embedding_layer(model.predict([["man"]]))

cosine_loss = tf.keras.losses.CosineSimilarity()
cosine_loss(A, B).numpy()

"""# GloVe"""

!wget "http://nlp.stanford.edu/data/glove.6B.zip"

!unzip "glove.6B.zip"

from gensim.models import KeyedVectors

glove_input_file = "glove.6B.100d.txt"

model = KeyedVectors.load_word2vec_format(glove_input_file, no_header=True)
model.most_similar("woman")

model["man"]

